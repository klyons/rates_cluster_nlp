{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from pandas.api.types import is_string_dtype, CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#authors note, different version of Keras put pad_sequences in different places, if one doesn't work, please try the other\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to load economic regimes found through unsupervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usa            0\n",
       "canada         2\n",
       "europe         0\n",
       "uk             0\n",
       "australia      0\n",
       "brazil         1\n",
       "india          1\n",
       "switzerland    0\n",
       "japan          0\n",
       "south korea    0\n",
       "Name: labels, dtype: int32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regimes = pd.read_parquet('quarterly_data/df_2020Q1.parquet')\n",
    "regimes['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = regimes[regimes['labels'] == 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the countries\n",
    "for country in countries:\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    if country == 'uk':\n",
    "        country = 'england'\n",
    "    if country == 'south korea':\n",
    "        country = 'korea'\n",
    "    df = pd.read_parquet(f\"fed_statements2/{country}.parquet\")\n",
    "    df = df.map(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x)) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Add the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames along the row axis\n",
    "data = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2019-11-13 00:01:00'),\n",
       " Timestamp('2020-02-11 00:01:00'),\n",
       " Timestamp('2020-05-13 00:01:00'),\n",
       " Timestamp('2020-06-16 00:01:00'),\n",
       " Timestamp('2020-08-27 00:11:00'),\n",
       " Timestamp('2020-10-07 00:02:00'),\n",
       " Timestamp('2021-02-23 00:01:00'),\n",
       " Timestamp('2021-06-22 00:04:00'),\n",
       " Timestamp('2021-07-14 00:01:00'),\n",
       " Timestamp('2021-09-02 00:11:00'),\n",
       " Timestamp('2021-10-12 00:00:00'),\n",
       " Timestamp('2021-11-28 00:01:00'),\n",
       " Timestamp('2021-12-02 00:01:00'),\n",
       " Timestamp('2022-03-02 00:01:00'),\n",
       " Timestamp('2022-03-22 00:09:00'),\n",
       " Timestamp('2022-06-01 00:02:00'),\n",
       " Timestamp('2022-06-20 00:02:00'),\n",
       " Timestamp('2022-06-23 00:02:00'),\n",
       " Timestamp('2022-08-26 00:01:00'),\n",
       " Timestamp('2022-09-30 00:01:00'),\n",
       " Timestamp('2022-12-01 00:08:00'),\n",
       " Timestamp('2023-01-11 00:02:00'),\n",
       " Timestamp('2023-03-08 00:01:00'),\n",
       " Timestamp('2023-06-22 00:01:00'),\n",
       " Timestamp('2023-07-03 00:02:00'),\n",
       " Timestamp('2023-08-30 00:08:00'),\n",
       " Timestamp('2023-10-11 00:06:00'),\n",
       " Timestamp('2023-10-20 00:01:00'),\n",
       " Timestamp('2023-10-26 00:02:00'),\n",
       " Timestamp('2023-10-27 00:01:00'),\n",
       " Timestamp('2023-11-07 00:07:00'),\n",
       " Timestamp('2023-11-13 00:08:00'),\n",
       " Timestamp('2023-11-15 00:06:00'),\n",
       " Timestamp('2023-11-20 00:02:00'),\n",
       " Timestamp('2023-11-21 00:03:00'),\n",
       " Timestamp('2024-03-06 00:01:00'),\n",
       " Timestamp('2020-03-12 00:01:00'),\n",
       " Timestamp('2020-04-30 00:02:00'),\n",
       " Timestamp('2020-05-07 00:02:00'),\n",
       " Timestamp('2020-05-08 00:01:00'),\n",
       " Timestamp('2020-05-25 00:04:00'),\n",
       " Timestamp('2020-06-04 00:01:00'),\n",
       " Timestamp('2020-06-09 00:02:00'),\n",
       " Timestamp('2020-06-11 00:02:00'),\n",
       " Timestamp('2020-06-15 00:02:00'),\n",
       " Timestamp('2020-07-17 00:01:00'),\n",
       " Timestamp('2020-09-10 00:01:00'),\n",
       " Timestamp('2020-09-14 00:02:00'),\n",
       " Timestamp('2020-09-21 00:02:00'),\n",
       " Timestamp('2020-09-30 00:03:00'),\n",
       " Timestamp('2020-10-19 00:01:00'),\n",
       " Timestamp('2020-10-29 00:01:00'),\n",
       " Timestamp('2020-12-11 00:02:00'),\n",
       " Timestamp('2021-01-21 00:02:00'),\n",
       " Timestamp('2021-03-02 00:10:00'),\n",
       " Timestamp('2021-03-11 00:02:00'),\n",
       " Timestamp('2021-03-18 00:02:00'),\n",
       " Timestamp('2021-04-14 00:01:00'),\n",
       " Timestamp('2021-06-10 00:02:00'),\n",
       " Timestamp('2021-06-22 00:02:00'),\n",
       " Timestamp('2021-07-02 00:07:00'),\n",
       " Timestamp('2021-09-28 00:02:00'),\n",
       " Timestamp('2021-11-07 00:04:00'),\n",
       " Timestamp('2021-11-16 00:02:00'),\n",
       " Timestamp('2021-11-23 00:02:00'),\n",
       " Timestamp('2021-11-30 00:08:00'),\n",
       " Timestamp('2021-12-08 00:02:00'),\n",
       " Timestamp('2021-12-17 00:02:00'),\n",
       " Timestamp('2022-01-03 00:02:00'),\n",
       " Timestamp('2022-01-21 00:02:00'),\n",
       " Timestamp('2022-02-04 00:01:00'),\n",
       " Timestamp('2022-02-10 00:01:00'),\n",
       " Timestamp('2022-02-17 00:02:00'),\n",
       " Timestamp('2022-02-18 00:05:00'),\n",
       " Timestamp('2022-03-03 00:03:00'),\n",
       " Timestamp('2022-03-10 00:01:00'),\n",
       " Timestamp('2022-03-16 00:01:00'),\n",
       " Timestamp('2022-04-14 00:04:00'),\n",
       " Timestamp('2022-04-22 00:01:00'),\n",
       " Timestamp('2022-04-25 00:04:00'),\n",
       " Timestamp('2022-04-28 00:04:00'),\n",
       " Timestamp('2022-05-02 00:02:00'),\n",
       " Timestamp('2022-05-06 00:08:00'),\n",
       " Timestamp('2022-05-13 00:04:00'),\n",
       " Timestamp('2022-06-09 00:04:00'),\n",
       " Timestamp('2022-06-21 00:01:00'),\n",
       " Timestamp('2022-06-29 00:03:00'),\n",
       " Timestamp('2022-07-05 00:01:00'),\n",
       " Timestamp('2022-07-21 00:02:00'),\n",
       " Timestamp('2022-08-30 00:02:00'),\n",
       " Timestamp('2022-09-08 00:01:00'),\n",
       " Timestamp('2022-09-21 00:03:00'),\n",
       " Timestamp('2022-09-26 00:01:00'),\n",
       " Timestamp('2022-10-03 00:01:00'),\n",
       " Timestamp('2022-10-17 00:02:00'),\n",
       " Timestamp('2022-10-27 00:01:00'),\n",
       " Timestamp('2022-11-04 00:03:00'),\n",
       " Timestamp('2022-11-15 00:02:00'),\n",
       " Timestamp('2022-11-21 00:03:00'),\n",
       " Timestamp('2022-11-29 00:02:00'),\n",
       " Timestamp('2022-12-20 00:02:00'),\n",
       " Timestamp('2023-02-02 00:02:00'),\n",
       " Timestamp('2023-02-16 00:02:00'),\n",
       " Timestamp('2023-03-16 00:02:00'),\n",
       " Timestamp('2023-03-21 00:01:00'),\n",
       " Timestamp('2023-03-22 00:03:00'),\n",
       " Timestamp('2023-04-04 00:01:00'),\n",
       " Timestamp('2023-04-18 00:03:00'),\n",
       " Timestamp('2023-05-04 00:02:00'),\n",
       " Timestamp('2023-06-01 00:03:00'),\n",
       " Timestamp('2023-06-06 00:02:00'),\n",
       " Timestamp('2023-06-19 00:06:00'),\n",
       " Timestamp('2023-06-20 00:05:00'),\n",
       " Timestamp('2023-06-28 00:03:00'),\n",
       " Timestamp('2023-07-11 00:06:00'),\n",
       " Timestamp('2023-07-31 00:02:00'),\n",
       " Timestamp('2023-09-01 00:05:00'),\n",
       " Timestamp('2023-09-14 00:02:00'),\n",
       " Timestamp('2023-09-25 00:02:00'),\n",
       " Timestamp('2023-10-06 00:01:00'),\n",
       " Timestamp('2023-10-17 00:02:00'),\n",
       " Timestamp('2023-10-26 00:02:00'),\n",
       " Timestamp('2023-11-08 00:03:00'),\n",
       " Timestamp('2023-11-13 00:01:00'),\n",
       " Timestamp('2023-11-22 00:02:00'),\n",
       " Timestamp('2023-11-27 00:06:00'),\n",
       " Timestamp('2023-12-04 00:02:00'),\n",
       " Timestamp('2023-12-05 00:02:00'),\n",
       " Timestamp('2024-01-17 00:02:00'),\n",
       " Timestamp('2024-02-08 00:05:00'),\n",
       " Timestamp('2024-02-14 00:02:00'),\n",
       " Timestamp('2024-02-16 00:03:00'),\n",
       " Timestamp('2023-11-07 00:11:00'),\n",
       " Timestamp('2023-11-20 00:09:00'),\n",
       " Timestamp('2019-09-25 00:07:00'),\n",
       " Timestamp('2019-10-01 00:02:00'),\n",
       " Timestamp('2019-11-26 00:06:00'),\n",
       " Timestamp('2020-02-06 00:08:00'),\n",
       " Timestamp('2020-02-11 00:02:00'),\n",
       " Timestamp('2020-03-19 00:02:00'),\n",
       " Timestamp('2020-04-21 00:02:00'),\n",
       " Timestamp('2020-05-28 00:00:00'),\n",
       " Timestamp('2020-06-04 00:06:00'),\n",
       " Timestamp('2020-07-17 00:09:00'),\n",
       " Timestamp('2020-08-03 00:06:00'),\n",
       " Timestamp('2020-08-10 00:07:00'),\n",
       " Timestamp('2020-11-24 00:07:00'),\n",
       " Timestamp('2020-12-04 00:02:00'),\n",
       " Timestamp('2021-02-04 00:03:00'),\n",
       " Timestamp('2021-02-16 00:02:00'),\n",
       " Timestamp('2021-03-10 00:08:00'),\n",
       " Timestamp('2021-05-06 00:13:00'),\n",
       " Timestamp('2021-06-17 00:08:00'),\n",
       " Timestamp('2021-07-09 00:08:00'),\n",
       " Timestamp('2021-08-09 00:02:00'),\n",
       " Timestamp('2021-09-15 00:07:00'),\n",
       " Timestamp('2021-11-16 00:09:00'),\n",
       " Timestamp('2022-02-02 00:07:00'),\n",
       " Timestamp('2022-02-11 00:02:00'),\n",
       " Timestamp('2022-02-23 00:03:00'),\n",
       " Timestamp('2022-05-04 00:01:00'),\n",
       " Timestamp('2022-05-23 00:05:00'),\n",
       " Timestamp('2022-06-21 00:06:00'),\n",
       " Timestamp('2022-09-08 00:05:00'),\n",
       " Timestamp('2022-09-16 00:02:00'),\n",
       " Timestamp('2022-10-19 00:04:00'),\n",
       " Timestamp('2022-10-24 00:05:00'),\n",
       " Timestamp('2022-11-09 00:05:00'),\n",
       " Timestamp('2022-11-22 00:05:00'),\n",
       " Timestamp('2023-02-17 00:02:00'),\n",
       " Timestamp('2023-03-08 00:08:00'),\n",
       " Timestamp('2023-04-05 00:09:00'),\n",
       " Timestamp('2023-04-20 00:01:00'),\n",
       " Timestamp('2023-06-20 00:06:00'),\n",
       " Timestamp('2023-07-12 00:03:00'),\n",
       " Timestamp('2023-08-14 00:02:00'),\n",
       " Timestamp('2023-09-07 00:03:00'),\n",
       " Timestamp('2023-10-11 00:04:00'),\n",
       " Timestamp('2023-10-26 00:05:00'),\n",
       " Timestamp('2023-11-15 00:04:00'),\n",
       " Timestamp('2023-11-27 00:05:00'),\n",
       " Timestamp('2024-02-09 00:02:00'),\n",
       " Timestamp('2020-04-20 00:01:00'),\n",
       " Timestamp('2020-05-07 00:02:00'),\n",
       " Timestamp('2020-07-15 00:02:00'),\n",
       " Timestamp('2020-11-06 00:09:00'),\n",
       " Timestamp('2020-12-17 00:02:00'),\n",
       " Timestamp('2021-04-30 00:08:00'),\n",
       " Timestamp('2022-06-20 00:02:00'),\n",
       " Timestamp('2022-09-22 00:01:00'),\n",
       " Timestamp('2022-11-18 00:13:00'),\n",
       " Timestamp('2022-12-19 00:02:00'),\n",
       " Timestamp('2023-09-21 00:02:00'),\n",
       " Timestamp('2023-11-02 00:01:00'),\n",
       " Timestamp('2023-11-15 00:10:00'),\n",
       " Timestamp('2023-11-20 00:02:00'),\n",
       " Timestamp('2023-12-15 00:03:00'),\n",
       " Timestamp('2020-02-05 00:12:00'),\n",
       " Timestamp('2020-02-06 00:06:00'),\n",
       " Timestamp('2020-05-26 00:01:00'),\n",
       " Timestamp('2020-08-03 00:09:00'),\n",
       " Timestamp('2020-08-31 00:11:00'),\n",
       " Timestamp('2020-09-02 00:12:00'),\n",
       " Timestamp('2020-09-23 00:05:00'),\n",
       " Timestamp('2020-10-30 00:13:00'),\n",
       " Timestamp('2020-11-04 00:05:00'),\n",
       " Timestamp('2020-11-19 00:07:00'),\n",
       " Timestamp('2020-11-25 00:01:00'),\n",
       " Timestamp('2020-12-04 00:07:00'),\n",
       " Timestamp('2020-12-08 00:00:00'),\n",
       " Timestamp('2020-12-23 00:12:00'),\n",
       " Timestamp('2021-02-04 00:12:00'),\n",
       " Timestamp('2021-02-18 00:10:00'),\n",
       " Timestamp('2021-03-12 00:08:00'),\n",
       " Timestamp('2021-05-17 00:01:00'),\n",
       " Timestamp('2021-05-19 00:08:00'),\n",
       " Timestamp('2022-03-11 00:09:00'),\n",
       " Timestamp('2022-04-20 00:03:00'),\n",
       " Timestamp('2022-06-03 00:11:00'),\n",
       " Timestamp('2022-11-18 00:01:00'),\n",
       " Timestamp('2022-11-21 00:08:00'),\n",
       " Timestamp('2022-12-14 00:10:00'),\n",
       " Timestamp('2023-02-02 00:13:00'),\n",
       " Timestamp('2023-03-01 00:10:00'),\n",
       " Timestamp('2023-03-08 00:09:00'),\n",
       " Timestamp('2023-06-28 00:09:00'),\n",
       " Timestamp('2023-07-12 00:11:00'),\n",
       " Timestamp('2023-08-03 00:10:00'),\n",
       " Timestamp('2023-09-06 00:13:00'),\n",
       " Timestamp('2023-09-12 00:08:00'),\n",
       " Timestamp('2023-09-25 00:07:00'),\n",
       " Timestamp('2023-10-02 00:13:00'),\n",
       " Timestamp('2023-11-07 00:07:00'),\n",
       " Timestamp('2023-11-20 00:01:00'),\n",
       " Timestamp('2023-12-07 00:01:00'),\n",
       " Timestamp('2023-12-13 00:10:00'),\n",
       " Timestamp('2023-12-27 00:09:00'),\n",
       " Timestamp('2024-02-08 00:12:00'),\n",
       " Timestamp('2020-08-17 00:04:00'),\n",
       " Timestamp('2021-01-08 00:05:00'),\n",
       " Timestamp('2022-01-13 00:01:00'),\n",
       " Timestamp('2022-06-10 00:03:00'),\n",
       " Timestamp('2022-07-13 00:05:00'),\n",
       " Timestamp('2022-10-28 00:07:00'),\n",
       " Timestamp('2022-11-25 00:01:00'),\n",
       " Timestamp('2023-01-10 00:01:00'),\n",
       " Timestamp('2024-02-14 00:02:00')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_parquet(\"data/bonds.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usa</th>\n",
       "      <th>canada</th>\n",
       "      <th>europe</th>\n",
       "      <th>uk</th>\n",
       "      <th>australia</th>\n",
       "      <th>brazil</th>\n",
       "      <th>india</th>\n",
       "      <th>switzerland</th>\n",
       "      <th>japan</th>\n",
       "      <th>south korea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>1.90</td>\n",
       "      <td>1.205238</td>\n",
       "      <td>0.0992</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>0.955</td>\n",
       "      <td>5.5882</td>\n",
       "      <td>6.62</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>1.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>1.50</td>\n",
       "      <td>1.364000</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.5981</td>\n",
       "      <td>1.035</td>\n",
       "      <td>6.3432</td>\n",
       "      <td>6.82</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>1.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.449545</td>\n",
       "      <td>0.1325</td>\n",
       "      <td>0.6373</td>\n",
       "      <td>1.037</td>\n",
       "      <td>4.4765</td>\n",
       "      <td>6.54</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>1.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.499500</td>\n",
       "      <td>0.3016</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>1.152</td>\n",
       "      <td>4.5261</td>\n",
       "      <td>6.64</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>1.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>1.78</td>\n",
       "      <td>1.604500</td>\n",
       "      <td>0.3662</td>\n",
       "      <td>0.8308</td>\n",
       "      <td>1.202</td>\n",
       "      <td>4.4842</td>\n",
       "      <td>6.85</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>1.653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             usa    canada  europe      uk  australia  brazil  india  \\\n",
       "2019-08-01  1.90  1.205238  0.0992  0.5776      0.955  5.5882   6.62   \n",
       "2019-09-01  1.50  1.364000  0.0478  0.5981      1.035  6.3432   6.82   \n",
       "2019-10-01  1.65  1.449545  0.1325  0.6373      1.037  4.4765   6.54   \n",
       "2019-11-01  1.73  1.499500  0.3016  0.7650      1.152  4.5261   6.64   \n",
       "2019-12-01  1.78  1.604500  0.3662  0.8308      1.202  4.4842   6.85   \n",
       "\n",
       "            switzerland  japan  south korea  \n",
       "2019-08-01       -0.975 -0.280        1.254  \n",
       "2019-09-01       -0.700 -0.215        1.420  \n",
       "2019-10-01       -0.511 -0.150        1.577  \n",
       "2019-11-01       -0.580 -0.080        1.750  \n",
       "2019-12-01       -0.456 -0.025        1.653  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_resampled = target['usa'].diff().resample('D').interpolate(method='polynomial', order=2)\n",
    "# Shift the DataFrame so we are predicting out in time\n",
    "target_shifted = target_resampled.shift(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shifted_selected = target_shifted[target_shifted.index.isin(data.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020-05-28   -0.017059\n",
       "2020-12-08    0.192532\n",
       "2021-10-12    0.135649\n",
       "Name: usa, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_shifted_selected.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-13 00:01:00</th>\n",
       "      <td>Jerome H Powell The economic outlook\\nTestimon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11 00:01:00</th>\n",
       "      <td>Jerome H Powell Semiannual Monetary Policy Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-13 00:01:00</th>\n",
       "      <td>Jerome H Powell Current economic issues\\nSpeec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-16 00:01:00</th>\n",
       "      <td>Jerome H Powell Semiannual Monetary Policy Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-27 00:11:00</th>\n",
       "      <td>For release on delivery  \\n am EDT  am CDT  \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-13 00:05:00</th>\n",
       "      <td>BOK Internatinoal Conference \\nOpening Address...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-28 00:07:00</th>\n",
       "      <td>October    \\n \\n \\n \\nKoreas Monetary Policy A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-25 00:01:00</th>\n",
       "      <td>BIS  Central bankers speechesChang Yong Rhee ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-10 00:01:00</th>\n",
       "      <td>BIS  Central bankers speechesChang Yong Rhee ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-14 00:02:00</th>\n",
       "      <td>BIS  Central bankers speechesChang Yong Rhee ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               content\n",
       "2019-11-13 00:01:00  Jerome H Powell The economic outlook\\nTestimon...\n",
       "2020-02-11 00:01:00  Jerome H Powell Semiannual Monetary Policy Rep...\n",
       "2020-05-13 00:01:00  Jerome H Powell Current economic issues\\nSpeec...\n",
       "2020-06-16 00:01:00  Jerome H Powell Semiannual Monetary Policy Rep...\n",
       "2020-08-27 00:11:00  For release on delivery  \\n am EDT  am CDT  \\n...\n",
       "...                                                                ...\n",
       "2022-07-13 00:05:00  BOK Internatinoal Conference \\nOpening Address...\n",
       "2022-10-28 00:07:00  October    \\n \\n \\n \\nKoreas Monetary Policy A...\n",
       "2022-11-25 00:01:00   BIS  Central bankers speechesChang Yong Rhee ...\n",
       "2023-01-10 00:01:00   BIS  Central bankers speechesChang Yong Rhee ...\n",
       "2024-02-14 00:02:00   BIS  Central bankers speechesChang Yong Rhee ...\n",
       "\n",
       "[247 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming df['content'] is a series of strings\n",
    "sentences = data['content'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words)).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "vocab_size = 10000 # max number of words\n",
    "embedding_dim = 120 # This is the dimension of the Word Vector\n",
    "max_length = 1000 # maximum length of the sequence\n",
    "trunc_type = 'post' # where to chop off\n",
    "padding_type = 'post' # where to put the padding\n",
    "oov_token = '<OOV>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# Check if a model is already saved\n",
    "model_path = \"models/fed_parser\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    word2vec = Word2Vec.load(model_path)\n",
    "    print(\"Loaded existing model\")\n",
    "else:\n",
    "    # Train your model\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token = oov_token)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    index_word = {v: k for k, v in word_index.items()}\n",
    "    sentences_tokenized = [[index_word[i] for i in seq] for seq in sequences]\n",
    "    word2vec = Word2Vec(sentences=sentences_tokenized, vector_size=120, window=5, min_count=1, workers=4)\n",
    "    word2vec.train(sentences_tokenized, total_examples=len(sentences_tokenized), epochs=10)\n",
    "    print(\"Trained a new model\")\n",
    "    # Save your model\n",
    "    word2vec.save(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save word vectors\n",
    "word_vectors = word2vec.wv.vectors\n",
    "\n",
    "# Create a word-to-index dictionary\n",
    "word_to_index = {word: i for i, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_to_index) + 1, word2vec.vector_size))\n",
    "for word, i in word_to_index.items():\n",
    "    embedding_matrix[i] = word2vec.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247, 1)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(target_shifted_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the common indexes\n",
    "common_indexes = data.index.intersection(target_shifted_selected.index)\n",
    "\n",
    "# Select the rows from both dataframes where the index is in common_indexes\n",
    "data = data[data.index.isin(common_indexes)]\n",
    "target_shifted_selected = target_shifted_selected[target_shifted_selected.index.isin(common_indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_features(full_array, window=20):\n",
    "\n",
    "    n_features = full_array.shape[1]\n",
    "    \n",
    "    expanded_features = np.zeros((full_array.shape[0], n_features * window))\n",
    "    for feature_idx in range(n_features):\n",
    "        # For each time step in the window\n",
    "        for lag in range(window):\n",
    "            # Shift the data\n",
    "            shifted_data = np.roll(full_array[:, feature_idx], lag)\n",
    "            # Set the first 'lag' values to zero\n",
    "            shifted_data[:lag] = 0\n",
    "            # Store it in the expanded features\n",
    "            expanded_features[:, feature_idx*window + lag] = shifted_data\n",
    "            \n",
    "    return expanded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['content'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "#data['sequences'] = tokenizer.texts_to_sequences(data['content'])\n",
    "sequences = tokenizer.texts_to_sequences(data['content'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_data = expand_features(padded_sequences, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg = Sequential()\n",
    "reg.add(Embedding(input_dim=len(word_to_index) + 1, output_dim=word2vec.vector_size, weights=[embedding_matrix]))\n",
    "reg.add(Bidirectional(LSTM(64, return_sequences = True)))\n",
    "reg.add(Dropout(0.25))\n",
    "reg.add(Bidirectional(LSTM(64, return_sequences = True)))\n",
    "reg.add(Dropout(0.25))\n",
    "reg.add(Bidirectional(LSTM(32)))\n",
    "reg.add(Dropout(0.25))\n",
    "reg.add(Dense(1))  # No activation function for regression\n",
    "\n",
    "# Compile the model with gradient clipping\n",
    "opt = Adam(clipvalue=0.5)\n",
    "reg.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy', 'mean_squared_error'])  # Mean squared error for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 3659s 3659s/step - loss: 0.4482 - val_loss: 0.3310\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 3487s 3487s/step - loss: 0.3959 - val_loss: 0.1432\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 3556s 3556s/step - loss: 0.1544 - val_loss: 0.0035\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 3642s 3642s/step - loss: 0.0493 - val_loss: 0.0725\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 3637s 3637s/step - loss: 7.4423e-04 - val_loss: 0.2621\n"
     ]
    }
   ],
   "source": [
    "history = reg.fit(expanded_data, target_shifted_selected.values, epochs=20, validation_split=0.2, batch_size=25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = Sequential()\n",
    "clf.add(Embedding(input_dim=len(word_to_index) + 1, output_dim=word2vec.vector_size, weights=[embedding_matrix]))\n",
    "clf.add(Bidirectional(LSTM(64, return_sequences = True)))\n",
    "clf.add(Bidirectional(LSTM(32)))\n",
    "clf.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Compile the model with gradient clipping\n",
    "opt = Adam(clipvalue=0.5)\n",
    "clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='rmse', patience=2)\n",
    "\n",
    "#history = clf.fit(expanded_data, target.values, epochs=10, validation_split=0.2, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 15s 15s/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [54, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Generate confusion matrix\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:319\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    225\u001b[0m     {\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    236\u001b[0m ):\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:430\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    428\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    433\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [54, 3]"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(expanded_data) \n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(target, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.10.11' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Visualize the loss\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.10.11' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Get the embeddings from the embedding layer\n",
    "embeddings = clf.layers[0].get_weights()[0]\n",
    "\n",
    "# Create a dictionary to map indices to words\n",
    "word_to_index = tokenizer.word_index\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "\n",
    "# Now you can get the embedding of a word like this:\n",
    "word = \"example\"\n",
    "word_embedding = embeddings[word_to_index[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.10.11' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.10.11' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
